#This is simple, step-by-step example shows the core functionality. 
from Preprocessor import BioinformaticsPreprocessor
from FeatureSelector import BioinformaticsFeatureSelector
from DimensionalityReducer import BioinformaticsDimensionalityReducer
from SupervisedLearner import BioinformaticsSupervisedLearner
from Clusstering import BioinformaticsClustering
from BioInterpreter import BiologicalInterpreter
from ModelEvaluator import BioinformaticsModelEvaluator
from ModelExplainer import BioinformaticsModelExplainer


# Initialize the preprocessor
preprocessor = BioinformaticsPreprocessor(data_type='rna_seq')

# Normalize RNA-seq data using log transformation
normalized_counts = preprocessor.preprocess(count_matrix, method='log')

# Initialize the feature selector with statistical method
selector = BioinformaticsFeatureSelector(method='statistical', task_type='classification')

# Fit the selector to identify differentially expressed genes
selector.fit(normalized_counts, condition_labels, test_type='f_test', k=50)

# Get selected features and their importance scores
selected_features = selector.get_selected_features()
feature_importances = selector.get_feature_importances()

# Apply selection to get reduced dataset
reduced_data = selector.transform(normalized_counts)

# Initialize dimensionality reducer
dim_reducer = BioinformaticsDimensionalityReducer(
    method='pca',    # One of: 'pca', 'lda', 'tsne', 'umap', 'autoencoder'
    n_components=2   # Number of dimensions to reduce to
)

# Fit and transform data
X_reduced = dim_reducer.fit_transform(normalized_data, labels)

# Visualize results
dim_reducer.plot_reduced_data(X_reduced=X_reduced, y=labels)

# For PCA, visualize explained variance
if dim_reducer.method == 'pca':
    dim_reducer.plot_explained_variance()
    
# Visualize feature loadings to interpret components
dim_reducer.plot_feature_loadings(n_features=20, component_idx=0)


# Initialize clusterer
clusterer = BioinformaticsClustering(
    method='kmeans',  # One of: 'kmeans', 'kmedoids', 'hierarchical', 'dbscan', 'gmm', 'wgcna', 'spectral'
    random_state=42
)

# Determine optimal number of clusters
optimization_results = clusterer.determine_optimal_clusters(
    normalized_data, 
    max_clusters=10,
    methods=['elbow', 'silhouette', 'calinski_harabasz']
)

# Get recommended number of clusters
n_clusters = optimization_results['recommendations']['overall']

# Perform clustering with optimal parameters
clusterer.fit(normalized_data, n_clusters=n_clusters)

# Evaluate clustering performance
metrics = clusterer.evaluate_clusters(normalized_data)
print(f"Silhouette score: {metrics['silhouette_score']:.3f}")

# Visualize clusters
clusterer.plot_clusters(normalized_data, method='pca')

# For hierarchical clustering, visualize dendrogram
if clusterer.method == 'hierarchical':
    clusterer.plot_dendrogram(normalized_data)

# For WGCNA, visualize the gene network
if clusterer.method == 'wgcna':
    clusterer.plot_wgcna_network()

# Initialize the supervised learner for classification
learner = BioinformaticsSupervisedLearner(
    task_type='classification',      # 'classification' or 'regression'
    model_type='random_forest',      # ML model types: 'random_forest', 'svm', 'xgboost', etc.
    random_state=42
)

# Train the model (with hyperparameter tuning)
learner.fit(X_train, y_train, 
           hyperparameter_tuning=True,
           param_grid={'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20]},
           cv=5)

# Make predictions
y_pred = learner.predict(X_test)
y_pred_proba = learner.predict_proba(X_test)  # For classification

# Evaluate model performance
metrics = learner.evaluate(X_test, y_test)
print(f"Accuracy: {metrics['accuracy']:.3f}")

# Visualize results
learner.plot_confusion_matrix()
learner.plot_roc_curve()
learner.plot_feature_importances(n_features=15)

# For deep learning models
if learner.model_type in ['mlp', 'cnn', 'rnn', 'transformer']:
    learner.plot_learning_curve()

# Save the model
learner.save_model('path/to/save/model')

# For sequence or pattern data
cnn_learner = BioinformaticsSupervisedLearner(
    task_type='classification',
    model_type='cnn'
)

# Configure CNN architecture
cnn_learner.fit(X_train, y_train,
               conv_layers=[(64, 3, 2), (128, 3, 2)],  # (filters, kernel_size, pool_size)
               hidden_layers=[128, 64],
               dropout_rate=0.3,
               batch_size=32,
               epochs=100)

# For temporal or sequential data
rnn_learner = BioinformaticsSupervisedLearner(
    task_type='regression',
    model_type='rnn'
)

# Configure RNN architecture with bidirectional LSTM
rnn_learner.fit(X_train, y_train,
               rnn_layers=[(64, True), (64, False)],  # (units, return_sequences)
               bidirectional=True,
               hidden_layers=[64])

# Initialize the evaluator
evaluator = BioinformaticsModelEvaluator(
    task_type='classification',  # 'classification', 'regression', or 'clustering'
    random_state=42
)

# Cross-validate a model
cv_results = evaluator.cross_validate_model(
    model, X, y,
    cv_strategy='stratified',
    n_splits=5, 
    scoring=['accuracy', 'roc_auc', 'f1_weighted']
)

# Optimize hyperparameters
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 5, 10, 20],
    'min_samples_split': [2, 5, 10]
}

opt_results = evaluator.optimize_hyperparameters(
    model, X, y,
    param_grid=param_grid,
    search_method='bayesian',  # 'grid', 'random', or 'bayesian'
    cv=5,
    n_iter=30  # for random or bayesian search
)

# Get the best parameters
best_params = evaluator.best_params_

# Evaluate on test data
metrics = evaluator.evaluate_model(
    model, X, y,
    test_size=0.2,
    scoring=['accuracy', 'precision', 'recall', 'f1', 'roc_auc']
)

# Calculate feature importance
importance_results = evaluator.permutation_importance(model, X, y, n_repeats=10)

# Generate visualizations
evaluator.plot_confusion_matrix()
evaluator.plot_roc_curve()
evaluator.plot_permutation_importance(n_features=15)

# Initialize the explainer with a trained model
explainer = BioinformaticsModelExplainer(
    model=trained_model,
    task_type='classification'  # or 'regression'
)

# Compute feature importance using permutation method
importance_df = explainer.compute_feature_importance(
    X_test, y_test, 
    method='permutation',
    n_repeats=10
)

# Plot feature importance
explainer.plot_feature_importance(n_features=15)

# Compute SHAP values for global and local explanations
explainer.compute_shap_values(
    X_test,
    background_samples=100
)

# Create SHAP summary plot
explainer.plot_shap_summary(plot_type='bar')

# Analyze how a specific feature impacts predictions
explainer.plot_shap_dependence(
    feature="gene_expression_BRCA1", 
    interaction_feature="mutation_count"
)

# Get LIME explanations for specific instances
explainer.compute_lime_explanation(
    X_test, 
    instances=[0, 1, 2],  # Explain first three instances
    num_features=10
)

# Visualize explanation for a particular instance
explainer.plot_lime_explanation(instance_index=0)

# For transformer models, extract and visualize attention weights
if explainer.model_type == 'transformer':
    explainer.compute_attention_weights(X_test)
    explainer.plot_attention_heatmap(instance_index=0)

# Generate a comprehensive explanation report with all methods
report = explainer.generate_explanation_report(
    X_test, y_test,
    output_format='html',
    n_samples=5,
    compute_shap=True,
    compute_lime=True,
    compute_attention=True
)

# Save the report
with open("model_explanation_report.html", "w") as f:
    f.write(report)


# In your complete pipeline class

class BioinformaticsPipeline:
    def __init__(self):
        # Initialize other pipeline components
        self.feature_selector = FeatureSelector()
        self.classifier = ClassificationModule()
        self.interpreter = BiologicalInterpreter(organism="human")
    
    def run_pipeline(self, data_matrix, labels):
        # Previous steps: preprocessing, normalization, etc.
        
        # Feature selection
        selected_features = self.feature_selector.select(data_matrix, labels)
        
        # Machine learning 
        model, performance = self.classifier.train(data_matrix, labels, selected_features)
        
        # Biological interpretation
        enrichment_results = self.interpreter.run_enrichment_analysis(
            gene_list=selected_features,
            source="KEGG"
        )
        
        return {
            "model": model,
            "performance": performance, 
            "biological_insights": enrichment_results
        }









